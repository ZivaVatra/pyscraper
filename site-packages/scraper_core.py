__VERSION__ = (1, 0, 0)

import mechanize

from ssl import SSLError, CertificateError
import ssl
from bs4 import BeautifulSoup as bs

import os

from random import choice

if os.path.exists("./user_agents"):
	with open("./user_agents") as fd:
		user_agents = [x.strip() for x in fd.readlines()]
else:
	user_agents = ["pyscraper_bot version %s" % ','.join([str(x) for x in __VERSION__])]


class scraper(object):
	""" Proxies format: { "http": "http_proxy_url", "ftp": "ftp_proxy_url" } """
	def __init__(
		self,
		rooturl,
		cache_html,
		ignore_ssl_errors=False,
		ignore_robots=True,
		user_agent=None,
		disable_http_refresh=True,
		proxies=None,
	):
		self.cache_html = cache_html
		self.Browser = mechanize.Browser()
		if proxies:
			self.Browser.set_proxies(proxies)
		# self.Browser.set_proxies( {
		#  "http": "joe:password@myproxy.example.com:3128",
		#  "ftp": "proxy.example.com",
		# } )

		if ignore_robots is True:
			# By default we act like a human as we have
			# human_user_agent string
			self.Browser.set_handle_robots(False)

		if user_agent is None:
			user_agent = choice(user_agents)

		self.Browser.addheaders = [
			('User-Agent', user_agent)
		]

		# Can sometimes hang without this.
		# It disables the <meta http-equiv="refresh" content="5">
		# which can cause hangs
		if disable_http_refresh is True:
			self.Browser.set_handle_refresh(False)

		self.rooturl = rooturl
		self.ignore_ssl_errors = ignore_ssl_errors

	def fetch_page(self, link=None, nocache=False):
		''' link can be either a string or a soup 'a' object.
			if it is a string, then it is hanled as the URL to fetch.
			if it is a soup object, we get the "href" attribute from it.
		'''
		if link is None:
			fullurl = self.rooturl
		elif type(link) is not str:
			fullurl = link.get("href")
			if not fullurl.startswith("http"):
				fullurl = link.base_url + '/' + fullurl
		else:
			if not link.startswith("http"):
				fullurl = self.rooturl + '/' + link
			else:
				fullurl = link

		if self.cache_html is not False and nocache is False:
			assert type(self.cache_html) is str,\
				"Please provide cache path as cache_html argument"
			if not os.path.exists(self.cache_html):
				print("Path %s does not exist. Creating." % self.cache_html)
				os.makedirs(self.cache_html)

			# for the cache filename + path
			file_url = fullurl.replace('http:', '')
			file_url = file_url.replace('https:', '')
			file_url = file_url.replace('/', '_')
			file_url = os.path.join(self.cache_html, file_url.strip() + ".html_cache")
			if os.path.exists(file_url):
				print(">>> Reading from cache")
				with open(file_url, 'r') as fd:
					# cached file exists, return without fetching remotely
					return bs(fd.read(), 'lxml')

		if self.ignore_ssl_errors is True:
			print("Disabling SSL verification...")
			try:
				_create_unverified_https_context = ssl._create_unverified_context
			except AttributeError:
				# Legacy Python that doesn't verify HTTPS certificates by default
				pass
			else:
				# Handle target environment that doesn't support HTTPS verification
				ssl._create_default_https_context = _create_unverified_https_context
			self.Browser.open(fullurl)

		try:
			self.Browser.open(fullurl)
		except (SSLError, CertificateError) as e:
			if self.ignore_ssl_errors is False:
				raise(e)
		except Exception as e:
			print("ERROR URL:", fullurl)
			raise(e)

		html = self.Browser.response().get_data()
		# Here we write out the HTML to a file for future caching/scraping
		if self.cache_html is not False and nocache is False:
			assert type(self.cache_html) is str,\
				"Please provide cache path as cache_html argument"
			with open(file_url, 'w') as fd:
				fd.write(html)
		return bs(html, "lxml")
